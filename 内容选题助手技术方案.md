# 内容选题助手技术方案文档

## 1. 技术架构概述

### 1.1 系统架构图
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   前端界面      │    │   API网关       │    │   大模型API     │
│   (React)       │◄──►│   (Express)     │◄──►│   (DeepSeek/国内) │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              │
                              ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │   核心服务      │◄──►│   数据采集服务   │
                       │   (Node.js)     │    │   (Python)      │
                       └─────────────────┘    └─────────────────┘
                              │                       │
                              ▼                       ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │   缓存层        │    │   消息队列      │
                       │   (Redis)       │    │   (RabbitMQ)    │
                       └─────────────────┘    └─────────────────┘
                              │                       │
                              ▼                       ▼
                       ┌─────────────────────────────────────────┐
                       │            数据存储层                    │
                       │   MongoDB (内容数据) + PostgreSQL (结构化数据) │
                       └─────────────────────────────────────────┘
```

### 1.2 技术栈选择

#### 前端技术栈
- **框架**: React 18 + TypeScript
- **UI组件库**: Ant Design
- **状态管理**: Redux Toolkit
- **路由**: React Router v6
- **HTTP客户端**: Axios
- **构建工具**: Vite
- **样式**: Tailwind CSS + Styled Components

#### 后端技术栈
- **主服务**: Node.js + Express + TypeScript
- **数据采集**: Python + FastAPI
- **API网关**: Express + http-proxy-middleware

- **API文档**: Swagger/OpenAPI

#### 大模型集成
- **主模型**: DeepSeek API
- **备选模型**: 豆包、阿里通义千问、讯飞星火
  - **模型路由**: 根据请求类型和负载自动选择最优模型

#### 数据存储
- **文档数据库**: MongoDB (存储爬取的原始内容)
- **关系数据库**: PostgreSQL (用户数据、分析结果)
- **缓存**: Redis (热点数据、API响应缓存)
- **搜索引擎**: Elasticsearch (内容检索)

#### 基础设施
- **容器化**: Docker + Docker Compose
- **消息队列**: RabbitMQ
- **任务调度**: Bull Queue (Node.js) / Celery (Python)
- **监控**: Prometheus + Grafana
- **日志**: Winston + ELK Stack

## 2. 核心模块设计

### 2.1 前端功能列表

#### 2.1.1 核心功能
- **智能关键词分析**：用户输入关键词，系统自动分析该关键词在各平台的热度、趋势和关联话题
- **多平台内容采集**：根据关键词自动采集知乎、B站、微博、小红书等平台的相关内容和评论
- **内容智能分析**：利用大模型对采集的内容进行深度分析，提取有价值的信息
- **选题建议生成**：基于分析结果，生成具体的选题建议和创作方向
- **内容大纲生成**：为每个选题建议生成详细的内容大纲和创作指导

#### 2.1.2 辅助功能
- **竞品分析**：分析同类内容的成功案例，提取可借鉴的创作元素
- **发布时机建议**：基于目标受众活跃时间，推荐最佳发布时机
- **内容形式建议**：根据选题特点和平台特性，推荐最合适的内容形式
- **历史记录与收藏**：保存用户的搜索历史和收藏的选题建议
- **用户画像分析**：分析关注该话题的用户特征，帮助精准定位目标受众

### 2.2 前端架构设计

#### 2.2.1 目录结构
```
frontend/
├── public/
│   ├── index.html
│   └── favicon.ico
├── src/
│   ├── components/          # 通用组件
│   │   ├── common/         # 基础组件
│   │   ├── forms/          # 表单组件
│   │   └── charts/         # 图表组件
│   ├── pages/              # 页面组件
│   │   ├── Home/           # 首页
│   │   ├── Search/         # 搜索页面
│   │   └── Analysis/       # 分析结果页面
│   ├── store/              # Redux状态管理
│   │   ├── slices/         # Redux切片
│   │   └── api/            # RTK Query API
│   ├── services/           # API服务
│   ├── utils/              # 工具函数
│   ├── hooks/              # 自定义Hooks
│   ├── types/              # TypeScript类型定义
│   └── styles/             # 全局样式
├── package.json
└── vite.config.ts
```

#### 2.2.2 核心组件设计

**KeywordSearch组件**
```typescript
interface KeywordSearchProps {
  onSearch: (keyword: string, platforms: string[]) => void;
  loading?: boolean;
}

const KeywordSearch: React.FC<KeywordSearchProps> = ({ onSearch, loading }) => {
  const [keyword, setKeyword] = useState('');
  const [platforms, setPlatforms] = useState(['zhihu']);
  
  const handleSearch = () => {
    if (keyword.trim()) {
      onSearch(keyword, platforms);
    }
  };
  
  return (
    <Card>
      <Input.Search
        placeholder="输入关键词搜索相关话题"
        value={keyword}
        onChange={(e) => setKeyword(e.target.value)}
        onSearch={handleSearch}
        loading={loading}
        enterButton="搜索"
      />
      <Checkbox.Group
        options={platformOptions}
        value={platforms}
        onChange={setPlatforms}
        style={{ marginTop: 16 }}
      />
    </Card>
  );
};
```

**AnalysisResult组件**
```typescript
interface AnalysisResultProps {
  data: AnalysisResult | null;
  loading: boolean;
}

const AnalysisResult: React.FC<AnalysisResultProps> = ({ data, loading }) => {
  if (loading) return <Spin size="large" />;
  if (!data) return <Empty description="暂无分析结果" />;
  
  return (
    <Tabs defaultActiveKey="directions">
      <TabPane tab="话题方向" key="directions">
        <List
          dataSource={data.topicDirections}
          renderItem={direction => (
            <List.Item>
              <Card size="small">{direction}</Card>
            </List.Item>
          )}
        />
      </TabPane>
      <TabPane tab="选题建议" key="suggestions">
        {data.topicSuggestions.map((suggestion, index) => (
          <SuggestionCard key={index} suggestion={suggestion} />
        ))}
      </TabPane>
      {/* 其他标签页 */}
    </Tabs>
  );
};
```

### 2.3 后端架构设计

#### 2.3.1 目录结构
```
backend/
├── src/
│   ├── controllers/        # 控制器
│   │   ├── search.controller.ts
│   │   └── analysis.controller.ts
│   ├── services/           # 业务逻辑服务
│   │   ├── search.service.ts
│   │   └── analysis.service.ts
│   ├── models/             # 数据模型
│   │   ├── search.model.ts
│   │   └── analysis.model.ts
│   ├── middleware/         # 中间件
│   │   ├── error.middleware.ts
│   │   └── rate-limit.middleware.ts
│   ├── routes/             # 路由定义
│   │   ├── search.routes.ts
│   │   └── analysis.routes.ts
│   ├── utils/              # 工具函数
│   ├── config/             # 配置文件
│   └── types/              # TypeScript类型定义
├── package.json
└── tsconfig.json
```

#### 2.3.2 核心服务设计

**内容分析服务**
```typescript
interface ContentItem {
  id: string;
  platform: string;
  title: string;
  content: string;
  author: string;
  publishTime: Date;
  url: string;
  comments: Comment[];
  metrics: Record<string, number>;
}

interface AnalysisResult {
  topicDirections: string[];
  userConcerns: string[];
  sentimentAnalysis: {
    positive: number;
    negative: number;
    neutral: number;
  };
  topicSuggestions: TopicSuggestion[];
}

class ContentAnalyzer {
  private deepseek: DeepSeekApi;
  private doubao: DoubaoApi;
  private redis: Redis;
  private logger: Logger;

  constructor(deepseekApiKey: string, doubaoApiKey: string, redisUrl: string, logger: Logger) {
    this.deepseek = new DeepSeekApi({ apiKey: deepseekApiKey });
    this.doubao = new DoubaoApi({ apiKey: doubaoApiKey });
    this.redis = new Redis(redisUrl);
    this.logger = logger;
  }

  async analyzeContent(keyword: string, contentItems: ContentItem[]): Promise<AnalysisResult> {
    // 检查缓存
    const cacheKey = this.getCacheKey(keyword, contentItems);
    const cached = await this.redis.get(cacheKey);
    if (cached) {
      this.logger.info(`返回缓存的分析结果: ${keyword}`);
      return JSON.parse(cached);
    }

    try {
      // 预处理内容
      const processedContent = this.preprocessContent(contentItems);
      
      // 调用大模型API
const analysisResult = await this.callModel(keyword, processedContent);
      
      // 缓存结果
      await this.redis.setex(cacheKey, 3600, JSON.stringify(analysisResult));
      
      return analysisResult;
    } catch (error) {
      this.logger.error(`内容分析失败: ${error.message}`);
      throw new Error(`内容分析失败: ${error.message}`);
    }
  }

  private preprocessContent(contentItems: ContentItem[]): string {
    // 限制内容数量以控制token消耗
    const articles = contentItems
      .filter(item => item.type === 'article')
      .slice(0, 10)
      .map(item => `${item.title}\n${item.content.substring(0, 500)}`)
      .join('\n\n');
    
    const comments = contentItems
      .flatMap(item => item.comments || [])
      .slice(0, 50)
      .map(comment => comment.text)
      .join('\n');
    
    return `文章内容：\n${articles}\n\n评论内容：\n${comments}`;
  }

  private async callModel(keyword: string, content: string): Promise<AnalysisResult> {
    // 根据负载和请求类型选择模型
    const model = this.selectOptimalModel();
    
    const prompt = `
    作为专业的内容分析师，请分析关键词"${keyword}"相关的内容，包括文章和评论。
    
    ${content}
    
    请提供JSON格式的分析结果，包含：
    1. topicDirections: 该话题下的主要讨论方向（最多5个）
    2. userConcerns: 用户关注点和痛点（最多5个）
    3. sentimentAnalysis: 情感倾向分析（正面、负面、中性的比例）
    4. topicSuggestions: 具体的选题建议（最多3个，每个包含标题、角度、原因）
    5. contentOutlines: 每个选题的简要内容大纲
    `;
    
    try {
      let response;
      
      if (model === 'deepseek') {
        response = await this.deepseek.createChatCompletion({
          model: 'deepseek-chat',
          messages: [
            { role: "system", content: "你是一个专业的内容分析师，擅长分析网络话题并提供选题建议。请始终以JSON格式返回结果。" },
            { role: "user", content: prompt }
          ],
          temperature: 0.3,
          max_tokens: 1500,
        });
      } else {
        // 使用豆包API
        response = await this.doubao.createChatCompletion({
          model: 'doubao-pro',
          messages: [
            { role: "system", content: "你是一个专业的内容分析师，擅长分析网络话题并提供选题建议。请始终以JSON格式返回结果。" },
            { role: "user", content: prompt }
          ],
          temperature: 0.3,
          max_tokens: 1500,
        });
      }
      
      return JSON.parse(response.data.choices[0].message.content);
    } catch (error) {
      this.logger.error('模型API调用失败:', error);
      throw new Error('内容分析失败');
    }
  }
  
  private selectOptimalModel(): string {
    // 简单的负载均衡逻辑，实际可以根据更复杂的策略
    const random = Math.random();
    return random < 0.7 ? 'deepseek' : 'doubao';
  }

  private getCacheKey(keyword: string, contentItems: ContentItem[]): string {
    const contentHash = crypto
      .createHash('md5')
      .update(JSON.stringify(contentItems.map(item => item.id)))
      .digest('hex');
    
    return `analysis:${keyword}:${contentHash}`;
  }
}
```

### 2.3 数据采集模块设计

#### 2.3.1 爬虫架构
```python
# content-crawler/src/main.py
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional
import asyncio
from datetime import datetime

app = FastAPI(title="内容采集服务")

class CrawlRequest(BaseModel):
    keyword: str
    platforms: List[str]  # ["zhihu", "bilibili", "weibo", ...]
    depth: int = 3  # 采集深度
    max_items: int = 100  # 最大采集数量
    time_range: Optional[int] = 30  # 时间范围（天）

class ContentItem(BaseModel):
    platform: str
    title: str
    content: str
    author: str
    publish_time: datetime
    url: str
    comments: List[Dict] = []
    metrics: Dict = {}  # 点赞、转发等数据

class CrawlResponse(BaseModel):
    task_id: str
    status: str
    message: str

# 平台爬虫工厂
class CrawlerFactory:
    @staticmethod
    def create_crawler(platform: str):
        if platform == "zhihu":
            return ZhihuCrawler()
        elif platform == "bilibili":
            return BilibiliCrawler()
        elif platform == "weibo":
            return WeiboCrawler()
        else:
            raise ValueError(f"不支持的平台: {platform}")

# 基础爬虫类
class BaseCrawler:
    async def crawl(self, keyword: str, max_items: int, time_range: int) -> List[ContentItem]:
        raise NotImplementedError
    
    def _clean_text(self, text: str) -> str:
        # 文本清洗逻辑
        pass
    
    def _extract_comments(self, item_data: Dict) -> List[Dict]:
        # 评论提取逻辑
        pass

# 知乎爬虫实现
class ZhihuCrawler(BaseCrawler):
    async def crawl(self, keyword: str, max_items: int, time_range: int) -> List[ContentItem]:
        # 实现知乎爬虫逻辑
        results = []
        
        # 搜索相关问题
        questions = await self._search_questions(keyword, max_items)
        
        # 获取问题详情和回答
        for question in questions:
            answers = await self._get_answers(question['id'])
            
            for answer in answers:
                # 提取评论
                comments = await self._get_comments(answer['id'])
                
                # 构建内容项
                content_item = ContentItem(
                    platform="zhihu",
                    title=question['title'],
                    content=answer['content'],
                    author=answer['author']['name'],
                    publish_time=datetime.fromisoformat(answer['created_time']),
                    url=f"https://www.zhihu.com/question/{question['id']}/answer/{answer['id']}",
                    comments=comments,
                    metrics={
                        "likes": answer['voteup_count'],
                        "comments": len(comments),
                        "thanks": answer['thanks_count']
                    }
                )
                
                results.append(content_item)
        
        return results
    
    async def _search_questions(self, keyword: str, max_items: int) -> List[Dict]:
        # 实现知乎问题搜索
        pass
    
    async def _get_answers(self, question_id: str) -> List[Dict]:
        # 获取问题下的回答
        pass
    
    async def _get_comments(self, answer_id: str) -> List[Dict]:
        # 获取回答下的评论
        pass

# API端点
@app.post("/crawl", response_model=CrawlResponse)
async def start_crawling(request: CrawlRequest, background_tasks: BackgroundTasks):
    task_id = f"crawl_{datetime.now().timestamp()}"
    
    # 将任务加入后台队列
    background_tasks.add_task(crawl_content, task_id, request)
    
    return CrawlResponse(
        task_id=task_id,
        status="queued",
        message="爬取任务已加入队列"
    )

@app.get("/crawl/{task_id}/status")
async def get_crawl_status(task_id: str):
    # 获取任务状态
    pass

@app.get("/crawl/{task_id}/results")
async def get_crawl_results(task_id: str):
    # 获取爬取结果
    pass

async def crawl_content(task_id: str, request: CrawlRequest):
    # 更新任务状态为"进行中"
    await update_task_status(task_id, "running")
    
    try:
        results = []
        
        # 根据平台选择不同的爬虫策略
        for platform in request.platforms:
            crawler = CrawlerFactory.create_crawler(platform)
            platform_results = await crawler.crawl(
                request.keyword, 
                request.max_items, 
                request.time_range
            )
            results.extend(platform_results)
        
        # 存储到MongoDB
        await save_to_mongodb(task_id, results)
        
        # 发送消息到队列，通知分析服务
        await send_to_queue("content_crawled", {
            "task_id": task_id,
            "keyword": request.keyword,
            "content_count": len(results)
        })
        
        # 更新任务状态为"完成"
        await update_task_status(task_id, "completed")
        
    except Exception as e:
        # 更新任务状态为"失败"
        await update_task_status(task_id, "failed", str(e))

async def save_to_mongodb(task_id: str, results: List[ContentItem]):
    # 存储到MongoDB
    pass

async def send_to_queue(queue_name: str, message: Dict):
    # 发送消息到队列
    pass

async def update_task_status(task_id: str, status: str, error_message: str = None):
    # 更新任务状态
    pass
```

### 2.4 数据模型设计

#### 2.4.1 MongoDB文档模型

**内容项模型**
```typescript
interface ContentItem {
  _id: ObjectId;
  taskId: string;
  platform: 'zhihu' | 'bilibili' | 'weibo' | 'xiaohongshu';
  title: string;
  content: string;
  author: {
    name: string;
    id: string;
    avatar?: string;
    followers?: number;
  };
  publishTime: Date;
  url: string;
  comments: Comment[];
  metrics: {
    likes: number;
    shares: number;
    comments: number;
    views?: number;
  };
  extractedAt: Date;
  processed: boolean; // 是否已被分析处理
}

interface Comment {
  id: string;
  author: string;
  content: string;
  publishTime: Date;
  likes: number;
  replies?: Comment[];
}
```

**分析结果模型**
```typescript
interface AnalysisResult {
  _id: ObjectId;
  taskId: string;
  keyword: string;
  platforms: string[];
  analysis: {
    topicDirections: string[];
    userConcerns: string[];
    sentimentAnalysis: {
      positive: number;
      negative: number;
      neutral: number;
    };
    topicSuggestions: TopicSuggestion[];
  };
  createdAt: Date;
  updatedAt: Date;
}

interface TopicSuggestion {
  title: string;
  angle: string;
  reason: string;
  outline: string[];
  targetAudience: string[];
  expectedImpact: string;
}
```

#### 2.4.2 PostgreSQL关系模型

**搜索任务表**
```sql
CREATE TABLE search_tasks (
  id SERIAL PRIMARY KEY,
  keyword VARCHAR(100) NOT NULL,
  platforms TEXT[], -- ['zhihu', 'bilibili']
  status VARCHAR(20) DEFAULT 'queued', -- queued, running, completed, failed
  content_count INTEGER DEFAULT 0,
  analysis_result_id INTEGER REFERENCES analysis_results(id),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**分析结果表**
```sql
CREATE TABLE analysis_results (
  id SERIAL PRIMARY KEY,
  task_id INTEGER REFERENCES search_tasks(id),
  keyword VARCHAR(100) NOT NULL,
  topic_directions JSONB,
  user_concerns JSONB,
  sentiment_analysis JSONB,
  topic_suggestions JSONB,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## 3. API设计

### 3.1 RESTful API规范



#### 3.1.1 搜索相关API
```
POST /api/search            # 发起本地搜索任务
GET  /api/search/:id        # 获取本地搜索任务详情
GET  /api/search/:id/status # 获取本地搜索任务状态
GET  /api/search/history    # 获取本地搜索历史（基于localStorage）
```

#### 3.1.2 分析相关API
```
GET  /api/analysis/:id      # 获取本地分析结果
POST /api/analysis/:id/share # 生成分享链接（本地生成）
GET  /api/analysis/:id/export # 导出分析结果（本地下载）
```



### 3.2 API响应格式

#### 3.2.1 成功响应
```json
{
  "success": true,
  "data": {
    // 具体数据
  },
  "message": "操作成功",
  "timestamp": "2023-07-20T12:00:00Z"
}
```

#### 3.2.2 错误响应
```json
{
  "success": false,
  "error": {
    "code": "INVALID_PARAMETER",
    "message": "参数无效",
    "details": {
      "field": "keyword",
      "reason": "关键词不能为空"
    }
  },
  "timestamp": "2023-07-20T12:00:00Z"
}
```

### 3.3 API限流策略

- **IP限流**: 每IP每小时20次搜索请求
- **全局限流**: 每分钟100次搜索请求

## 4. 本地部署方案

### 4.1 本地部署架构

本地部署方案允许用户在自己的服务器或本地计算机上运行完整的内容选题助手系统，确保数据完全可控，并减少对云服务的依赖。

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   前端界面      │    │   API网关       │    │   本地大模型    │
│   (React)       │◄──►│   (Express)     │◄──►│   (DeepSeek)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              │
                              ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │   核心服务      │◄──►│   数据采集服务   │
                       │   (Node.js)     │    │   (Python)      │
                       └─────────────────┘    └─────────────────┘
                              │                       │
                              ▼                       ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │   缓存层        │    │   消息队列      │
                       │   (Redis)       │    │   (RabbitMQ)    │
                       └─────────────────┘    └─────────────────┘
                              │                       │
                              ▼                       ▼
                       ┌─────────────────────────────────────────┐
                       │            本地数据存储层                │
                       │   MongoDB (内容数据) + PostgreSQL (结构化数据) │
                       └─────────────────────────────────────────┘
```

### 4.2 本地大模型部署

#### 4.2.1 DeepSeek本地部署

DeepSeek提供了开源模型，可以在本地部署：

```bash
# 1. 安装必要的依赖
pip install torch transformers accelerate

# 2. 下载并运行DeepSeek模型
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base")
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-6.7b-base",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# 3. 创建API服务
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()

class ChatRequest(BaseModel):
    messages: list
    temperature: float = 0.7
    max_tokens: int = 1500

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    # 格式化输入
    prompt = tokenizer.apply_chat_template(
        request.messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # 生成响应
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=request.max_tokens,
        temperature=request.temperature,
        do_sample=True,
    )
    
    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    return {
        "choices": [{
            "message": {
                "content": response_text,
                "role": "assistant"
            }
        }]
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

#### 4.2.2 Docker容器化部署

创建本地大模型服务的Dockerfile：

```dockerfile
# local-llm/Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# 安装Python和依赖
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /app

# 安装Python依赖
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 暴露端口
EXPOSE 8080

# 启动命令
CMD ["python3", "app.py"]
```

requirements.txt内容：
```
torch>=2.0.0
transformers>=4.30.0
accelerate>=0.20.0
fastapi>=0.100.0
uvicorn>=0.22.0
pydantic>=1.10.0
```

### 4.3 本地Docker Compose配置

创建适用于本地部署的Docker Compose文件：

```yaml
# docker-compose.local.yml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - api-gateway
    restart: unless-stopped

  api-gateway:
    build: ./backend
    ports:
      - "5000:5000"
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - MONGODB_URL=mongodb://mongodb:27017/content-hub
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/content-hub
      - LLM_API_URL=http://local-llm:8080/v1/chat/completions
      - LLM_API_KEY=local-deployment
    depends_on:
      - redis
      - mongodb
      - postgres
      - local-llm
    restart: unless-stopped

  content-crawler:
    build: ./content-crawler
    environment:
      - MONGODB_URL=mongodb://mongodb:27017/content-hub
      - RABBITMQ_URL=amqp://rabbitmq:5672
    depends_on:
      - mongodb
      - rabbitmq
    restart: unless-stopped

  content-analyzer:
    build: ./content-analyzer
    environment:
      - LLM_API_URL=http://local-llm:8080/v1/chat/completions
      - LLM_API_KEY=local-deployment
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://rabbitmq:5672
    depends_on:
      - local-llm
      - redis
      - rabbitmq
    restart: unless-stopped

  local-llm:
    build: ./local-llm
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models  # 挂载模型文件目录
    environment:
      - MODEL_NAME=deepseek-ai/deepseek-coder-6.7b-base
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  mongodb:
    image: mongo:5.0
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
      - ./mongodb/init:/docker-entrypoint-initdb.d
    restart: unless-stopped

  postgres:
    image: postgres:14
    environment:
      - POSTGRES_DB=content-hub
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    restart: unless-stopped

volumes:
  mongodb_data:
  postgres_data:
  redis_data:
  rabbitmq_data:
```

### 4.4 本地部署脚本

创建一个自动化部署脚本，简化本地部署过程：

```bash
#!/bin/bash
# deploy-local.sh

echo "开始部署内容选题助手本地版..."

# 检查Docker和Docker Compose是否安装
if ! command -v docker &> /dev/null; then
    echo "错误: Docker未安装，请先安装Docker"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "错误: Docker Compose未安装，请先安装Docker Compose"
    exit 1
fi

# 创建必要的目录
mkdir -p models
mkdir -p mongodb/init
mkdir -p postgres/init

# 下载模型（如果本地没有）
if [ ! -d "models/deepseek-coder" ]; then
    echo "正在下载DeepSeek模型..."
    mkdir -p models/deepseek-coder
    # 这里可以使用git lfs或其他方式下载模型
    # git clone https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base models/deepseek-coder
fi

# 构建并启动服务
echo "构建并启动服务..."
docker-compose -f docker-compose.local.yml up --build -d

# 等待服务启动
echo "等待服务启动..."
sleep 30

# 检查服务状态
echo "检查服务状态..."
docker-compose -f docker-compose.local.yml ps

# 初始化数据库
echo "初始化数据库..."
docker-compose -f docker-compose.local.yml exec api-gateway npm run db:migrate

echo "部署完成！"
echo "前端地址: http://localhost"
echo "API文档: http://localhost:5000/api-docs"
echo "RabbitMQ管理界面: http://localhost:15672 (用户名: admin, 密码: password)"
```

### 4.5 本地部署优化

#### 4.5.1 模型量化

为了在资源有限的环境中运行大模型，可以使用模型量化技术：

```python
# 在local-llm/app.py中添加模型量化支持
import torch
from transformers import BitsAndBytesConfig

# 配置量化
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-6.7b-base",
    quantization_config=quantization_config,
    device_map="auto"
)
```

#### 4.5.2 模型缓存优化

```python
# 添加模型缓存优化
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

# 设置缓存目录
cache_dir = os.environ.get("MODEL_CACHE_DIR", "/app/models/cache")

tokenizer = AutoTokenizer.from_pretrained(
    "deepseek-ai/deepseek-coder-6.7b-base",
    cache_dir=cache_dir
)

model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/deepseek-coder-6.7b-base",
    cache_dir=cache_dir,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

### 4.6 本地部署监控

#### 4.6.1 系统监控

```yaml
# 添加到docker-compose.local.yml
  monitoring:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped

volumes:
  # ... 其他卷
  grafana_data:
```

#### 4.6.2 日志聚合

```yaml
# 添加到docker-compose.local.yml
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: unless-stopped

  logstash:
    image: docker.elastic.co/logstash/logstash:7.15.0
    volumes:
      - ./monitoring/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  # ... 其他卷
  elasticsearch_data:
```

### 4.7 本地部署维护

#### 4.7.1 备份脚本

```bash
#!/bin/bash
# backup.sh

BACKUP_DIR="/backup/content-hub"
DATE=$(date +%Y%m%d_%H%M%S)

# 创建备份目录
mkdir -p $BACKUP_DIR

# 备份MongoDB
docker-compose -f docker-compose.local.yml exec -T mongodb mongodump --out /tmp/backup
docker cp $(docker-compose -f docker-compose.local.yml ps -q mongodb):/tmp/backup $BACKUP_DIR/mongodb_$DATE

# 备份PostgreSQL
docker-compose -f docker-compose.local.yml exec -T postgres pg_dump -U postgres content-hub > $BACKUP_DIR/postgres_$DATE.sql

# 压缩备份
tar -czf $BACKUP_DIR/backup_$DATE.tar.gz $BACKUP_DIR/mongodb_$DATE $BACKUP_DIR/postgres_$DATE.sql

# 清理临时文件
rm -rf $BACKUP_DIR/mongodb_$DATE $BACKUP_DIR/postgres_$DATE.sql

# 保留最近7天的备份
find $BACKUP_DIR -name "backup_*.tar.gz" -mtime +7 -delete

echo "备份完成: $BACKUP_DIR/backup_$DATE.tar.gz"
```

#### 4.7.2 更新脚本

```bash
#!/bin/bash
# update.sh

echo "开始更新内容选题助手..."

# 备份当前数据
./backup.sh

# 拉取最新代码
git pull origin main

# 重新构建并启动服务
docker-compose -f docker-compose.local.yml down
docker-compose -f docker-compose.local.yml build --no-cache
docker-compose -f docker-compose.local.yml up -d

# 等待服务启动
sleep 30

# 运行数据库迁移
docker-compose -f docker-compose.local.yml exec api-gateway npm run db:migrate

echo "更新完成！"
```

## 5. 云服务部署

### 5.1 Docker容器化

#### 5.1.1 前端Dockerfile
```dockerfile
# frontend/Dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

#### 5.1.2 后端Dockerfile
```dockerfile
# backend/Dockerfile
FROM node:18-alpine

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

EXPOSE 5000
CMD ["node", "dist/index.js"]
```

#### 5.1.3 爬虫服务Dockerfile
```dockerfile
# content-crawler/Dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.2 Docker Compose配置

```yaml
# docker-compose.yml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - api-gateway
    restart: unless-stopped

  api-gateway:
    build: ./backend
    ports:
      - "5000:5000"
    environment:
      - NODE_ENV=production
      - REDIS_URL=redis://redis:6379
      - MONGODB_URL=mongodb://mongodb:27017/content-hub
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/content-hub
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - DOUBAO_API_KEY=${DOUBAO_API_KEY}
    depends_on:
      - redis
      - mongodb
      - postgres
    restart: unless-stopped

  content-crawler:
    build: ./content-crawler
    environment:
      - MONGODB_URL=mongodb://mongodb:27017/content-hub
      - RABBITMQ_URL=amqp://rabbitmq:5672
    depends_on:
      - mongodb
      - rabbitmq
    restart: unless-stopped

  content-analyzer:
    build: ./content-analyzer
    environment:
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - DOUBAO_API_KEY=${DOUBAO_API_KEY}
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://rabbitmq:5672
    depends_on:
      - redis
      - rabbitmq
    restart: unless-stopped

  mongodb:
    image: mongo:5.0
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    restart: unless-stopped

  postgres:
    image: postgres:14
    environment:
      - POSTGRES_DB=content-hub
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    restart: unless-stopped

volumes:
  mongodb_data:
  postgres_data:
  redis_data:
  rabbitmq_data:
```

### 5.3 云服务部署

#### 5.3.1 AWS部署架构
- **前端**: AWS S3 + CloudFront
- **后端**: AWS ECS (Fargate)
- **数据库**: AWS RDS (PostgreSQL) + DocumentDB (MongoDB)
- **缓存**: AWS ElastiCache (Redis)
- **消息队列**: AWS SQS + SNS
- **负载均衡**: AWS Application Load Balancer
- **监控**: AWS CloudWatch

#### 5.3.2 阿里云部署架构
- **前端**: 阿里云OSS + CDN
- **后端**: 阿里云ECS + 容器服务
- **数据库**: 阿里云RDS (PostgreSQL) + MongoDB
- **缓存**: 阿里云Redis
- **消息队列**: 阿里云消息队列MQ
- **负载均衡**: 阿里云SLB
- **监控**: 阿里云云监控

## 6. 性能优化

### 6.1 前端优化

1. **代码分割**: 使用React.lazy和Suspense实现路由级别的代码分割
2. **组件懒加载**: 对大型组件使用懒加载
3. **资源优化**: 图片压缩、字体子集化
4. **缓存策略**: 合理设置浏览器缓存和CDN缓存
5. **性能监控**: 使用Web Vitals监控页面性能

### 6.2 后端优化

1. **数据库优化**:
   - 合理设计索引
   - 使用连接池
   - 读写分离
   - 分页查询

2. **缓存策略**:
   - Redis缓存热点数据
   - 分析结果缓存
   - 用户会话缓存

3. **API优化**:
   - 请求合并
   - 数据压缩
   - 异步处理

4. **任务队列**:
   - 长时间任务异步化
   - 优先级队列
   - 失败重试机制

### 6.3 爬虫优化

1. **并发控制**: 限制并发请求数量
2. **请求优化**: 
   - 请求头伪装
   - 请求间隔随机化
   - 失败重试策略

3. **数据优化**:
   - 增量更新
   - 数据去重
   - 内容压缩

## 7. 安全方案

### 7.1 API安全

1. **API限流**: 防止API滥用
2. **请求验证**: 参数校验和格式验证

### 7.2 数据安全

1. **数据加密**: 敏感数据加密存储
2. **传输安全**: HTTPS加密传输
3. **SQL注入防护**: 使用参数化查询
4. **XSS防护**: 输入输出过滤

### 7.3 爬虫合规

1. **robots.txt**: 遵守网站爬虫协议
2. **请求频率**: 控制请求频率，避免对目标网站造成压力
3. **数据使用**: 仅用于分析，不用于其他商业用途
4. **用户隐私**: 不采集和存储用户个人隐私信息

## 8. 监控与日志

### 8.1 应用监控

1. **性能监控**: 响应时间、吞吐量、错误率
2. **资源监控**: CPU、内存、磁盘、网络使用率
3. **业务监控**: 用户活跃度、功能使用率、转化率

### 8.2 日志管理

1. **日志收集**: 使用ELK Stack收集日志
2. **日志分级**: ERROR、WARN、INFO、DEBUG
3. **日志分析**: 异常检测、趋势分析
4. **日志告警**: 关键错误实时告警

### 8.3 告警机制

1. **系统告警**: 服务宕机、资源不足
2. **业务告警**: 分析失败率高、用户投诉增加
3. **通知方式**: 邮件、短信、钉钉/企业微信

## 9. 测试策略

### 9.1 前端测试

1. **单元测试**: Jest + React Testing Library
2. **集成测试**: 组件交互测试
3. **E2E测试**: Cypress
4. **性能测试**: Lighthouse CI

### 9.2 后端测试

1. **单元测试**: Jest
2. **集成测试**: Supertest
3. **API测试**: Postman/Newman
4. **性能测试**: Artillery

### 9.3 爬虫测试

1. **单元测试**: 爬虫逻辑测试
2. **集成测试**: 端到端爬取测试
3. **稳定性测试**: 长时间运行测试
4. **合规测试**: robots.txt遵守测试

## 10. 运维方案

### 10.1 持续集成/持续部署

1. **代码管理**: Git + GitLab/GitHub
2. **CI/CD工具**: Jenkins/GitLab CI
3. **自动化测试**: 代码提交触发测试
4. **自动化部署**: 测试通过自动部署

### 10.2 容灾备份

1. **数据备份**: 定期数据库备份
2. **服务冗余**: 多实例部署
3. **故障转移**: 自动故障切换
4. **恢复演练**: 定期灾备演练

### 10.3 扩容方案

1. **水平扩展**: 增加服务实例
2. **垂直扩展**: 提升单机性能
3. **自动扩容**: 基于负载自动扩容
4. **弹性伸缩**: 按需分配资源

## 11. 开发计划

### 11.1 MVP开发计划（1-2个月）

**第一周**:
- 项目初始化和基础架构搭建
- 前端基础框架和路由配置
- 后端基础框架和API设计

**第二周**:
- 关键词搜索界面开发
- 知乎爬虫基础功能开发

**第三周**:
- 内容分析服务开发
- DeepSeek API集成
- 分析结果展示界面开发

**第四周**:
- 基础选题建议生成
- 内容大纲生成
- 系统集成测试

**第五周**:
- 性能优化和bug修复
- 用户体验优化
- 部署准备

**第六周**:
- 生产环境部署
- 监控和日志系统配置
- MVP发布和测试

### 11.2 迭代开发计划

**一期迭代（2-3个月）**:
- B站和微博爬虫开发
- 情感分析功能
- 性能优化

**二期迭代（3-4个月）**:
- 竞品分析功能
- 发布时机和内容形式建议
- 用户画像分析
- 移动端适配

**三期迭代（4-6个月）**:
- 小红书、抖音等平台支持
- API开放平台
- 团队协作功能
- 个性化推荐系统

## 12. 风险评估与应对

### 12.1 技术风险

1. **爬虫被封**:
   - 风险: IP被封，爬虫失效
   - 应对: 代理池、请求限流、多账号轮换

2. **API限制**:
   - 风险: DeepSeek API限制或成本过高
   - 应对: 多模型备选、结果缓存、智能降级

3. **性能瓶颈**:
   - 风险: 大量并发请求导致系统崩溃
   - 应对: 负载均衡、缓存优化、异步处理

### 12.2 业务风险

1. **数据合规**:
   - 风险: 数据采集违反平台规定
   - 应对: 遵守robots.txt、仅采集公开数据

2. **用户隐私**:
   - 风险: 用户数据泄露
   - 应对: 数据加密、访问控制、安全审计

3. **内容质量**:
   - 风险: 分析结果不准确，用户不满意
   - 应对: 用户反馈机制、结果优化、人工审核

### 12.3 运营风险

1. **成本控制**:
   - 风险: API调用成本过高
   - 应对: 智能缓存、请求优化、分级定价

2. **竞争压力**:
   - 风险: 竞争对手推出类似产品
   - 应对: 快速迭代、功能创新、用户体验

## 13. 总结

本技术方案详细描述了内容选题助手产品的技术架构、实现方案和开发计划。通过前后端分离、微服务架构、容器化部署等现代技术实践，确保系统的可扩展性、可维护性和高可用性。

在开发过程中，我们将采用敏捷开发方法，快速迭代，持续优化，确保产品能够满足用户需求，并在竞争中保持优势。同时，我们也将重视数据安全和合规性，确保产品的可持续发展。

通过本技术方案的实施，我们有信心打造出一款高质量、高性能的内容选题助手产品，为内容创作者提供强大的选题支持。